{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d95ac77",
   "metadata": {},
   "source": [
    "## CYGNSS Okavango: Regional Aggregation and Front-Normal Velocity Tools\n",
    "\n",
    "This version centralizes imports and utilities, and validates against nbformat schema.\n",
    "\n",
    "\n",
    "- Merges daily CYGNSS Okavango NetCDF tiles into a single time–lat–lon DataArray, with quick sanity checks and exploratory plots.\n",
    "\n",
    "- Provides reusable utilities for DSWE-style wet masks, regionmask polygons, and monthly composites over user-defined regions.\n",
    "\n",
    "- Computes wet/dry front-normal and channel-parallel velocities between two dates to analyze Okavango flood-pulse dynamics along channels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e50d194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined imports (deduplicated)\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "from pathlib import Path\n",
    "from pyproj import Transformer\n",
    "from shapely.geometry import box, LineString, MultiLineString, Point\n",
    "from shapely.ops import linemerge\n",
    "from typing import Iterable, List, Tuple\n",
    "import calendar\n",
    "import ee\n",
    "import geemap\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import regionmask\n",
    "import warnings\n",
    "import xarray as xr\n",
    "\n",
    "from src.gee_utils import ee_init\n",
    "from src.figures import save_figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd91b04",
   "metadata": {
    "code_folding": [
     26
    ]
   },
   "outputs": [],
   "source": [
    "# Import analysis utilities from the shared module\n",
    "# (replaces ~1160 lines of inline function definitions)\n",
    "from src.okavango_fronts import (\n",
    "    _ensure_time_coord,\n",
    "    _find_lat_lon_names,\n",
    "    _infer_mask,\n",
    "    _normalize_dims,\n",
    "    _open_dataset_robust,\n",
    "    _pick_var_with_latlon,\n",
    "    classify_front_direction,\n",
    "    front_normal_velocity,\n",
    "    front_speed_along_channels,\n",
    "    plot_parallel_velocity_with_channels,\n",
    "    plot_three_panel,\n",
    "    stats,\n",
    "    velocity_parallel_to_nearest_channel_field,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c04105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NC_PATH   = Path(\"../data/cygnss_okavango_daily/cygnss_okavango_daily_merged.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472cf262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Merge individual files (NetCDFs) directly into one DataArray ----------------\n",
    "# ====== USER SETTINGS ======\n",
    "INPUT_DIR   = Path('../data/cygnss_okavango_daily/')             # folder containing per-file NetCDFs\n",
    "GLOB        = \"*.nc\"                         # glob to match files\n",
    "VAR_NAME    = None                           # set to variable name (str) to force; or None to auto-pick\n",
    "CHUNKS      = {\"time\": -1, \"lat\": 512, \"lon\": 512}  # tweak for your data/compute\n",
    "\n",
    "# Override module-level config for this dataset's conventions\n",
    "import src.okavango_fronts as _of\n",
    "_of.ENGINE_TRY  = (\"netcdf4\", \"h5netcdf\", \"scipy\")\n",
    "_of.STRICT_TIME_FROM_FILENAME = False            # set True if files lack a 'time' coord\n",
    "_of.TIME_REGEX  = r\"(\\d{4})(\\d{2})(\\d{2})\"       # used only if STRICT_TIME_FROM_FILENAME is True\n",
    "\n",
    "# ====== Collect, open, normalize, and concat ======\n",
    "files = sorted(INPUT_DIR.glob(GLOB))\n",
    "if not files:\n",
    "    raise FileNotFoundError(f\"No files matched {INPUT_DIR / GLOB}\")\n",
    "parts = []\n",
    "skipped = 0\n",
    "for p in files:\n",
    "    try:\n",
    "        ds_i = _open_dataset_robust(p)\n",
    "        # find names\n",
    "        lat_name, lon_name = _find_lat_lon_names(ds_i)\n",
    "        varname = VAR_NAME or _pick_var_with_latlon(ds_i, lat_name, lon_name)\n",
    "        da_i = ds_i[varname]\n",
    "        # ensure time exists (singleton if needed)\n",
    "        time_name = next((d for d in da_i.dims if d.lower() == \"time\"), None)\n",
    "        if time_name is None:\n",
    "            da_i = _ensure_time_coord(da_i, p)\n",
    "            time_name = \"time\"\n",
    "        # normalize dims and optionally chunk\n",
    "        da_i = _normalize_dims(da_i, time_name, lat_name, lon_name)\n",
    "        if CHUNKS:\n",
    "            da_i = da_i.chunk(CHUNKS)\n",
    "        parts.append(da_i)\n",
    "    except Exception as e:\n",
    "        skipped += 1\n",
    "        warnings.warn(f\"Skipping {p.name}: {e}\")\n",
    "if not parts:\n",
    "    raise RuntimeError(\"All files failed to open or contained no usable variables.\")\n",
    "# Concatenate over time (auto-align lat/lon by coordinate equality)\n",
    "da = xr.concat(parts, dim=\"time\", join=\"exact\")  # change join to 'override' or 'outer' if grids differ slightly\n",
    "# Optional: sort by time & drop duplicate NaT rows if dummy times were used\n",
    "if da.indexes.get(\"time\", None) is not None:\n",
    "    da = da.sortby(\"time\")\n",
    "    if np.issubdtype(da.time.dtype, np.datetime64):\n",
    "        # drop exact duplicates in time if any\n",
    "        _, unique_idx = np.unique(da.time.values, return_index=True)\n",
    "        if len(unique_idx) != da.sizes[\"time\"]:\n",
    "            da = da.isel(time=np.sort(unique_idx))\n",
    "print(f\"Merged {len(parts)} files; skipped {skipped}.\")\n",
    "print(\"Final shape:\", dict(da.sizes))   # {'time': ..., 'lat': ..., 'lon': ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f530d8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "windows = [\n",
    "    (\"2019-01-01\", \"2019-05-01\", \"2019 Jan–May\"),\n",
    "    (\"2020-01-01\", \"2020-05-01\", \"2020 Jan–May\"),\n",
    "    (\"2021-01-01\", \"2021-05-01\", \"2021 Jan–May\"),\n",
    "]\n",
    "\n",
    "# compute v_normal for each window\n",
    "v_list = []\n",
    "for start, end, _ in windows:\n",
    "    ds = front_normal_velocity(\n",
    "        da, start, end,\n",
    "        front_value=0.5,\n",
    "        bandwidth=0.05,\n",
    "        smooth_px=1\n",
    "    )\n",
    "    v_list.append(ds[\"v_normal\"])\n",
    "\n",
    "# shared, robust, symmetric limits across all panels\n",
    "all_vals = np.concatenate([v.values.ravel() for v in v_list])\n",
    "all_vals = all_vals[np.isfinite(all_vals)]\n",
    "lim = np.quantile(np.abs(all_vals), 0.98)   # tweak (e.g., 0.95) or set lim=30 manually\n",
    "norm = mcolors.TwoSlopeNorm(vmin=-lim, vcenter=0.0, vmax=lim)\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    1, 3, figsize=(15, 5),\n",
    "    sharex=True, sharey=True,\n",
    "    constrained_layout=True\n",
    ")\n",
    "\n",
    "for ax, v, (_, _, title) in zip(axes, v_list, windows):\n",
    "    im = v.plot(\n",
    "        ax=ax,\n",
    "        cmap=\"RdBu_r\",\n",
    "        norm=norm,\n",
    "        add_colorbar=False\n",
    "    )\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"lon\")\n",
    "\n",
    "axes[0].set_ylabel(\"lat\")\n",
    "for ax in axes[1:]:\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "cbar = fig.colorbar(im, ax=axes, pad=0.02, shrink=0.9)\n",
    "cbar.set_label(\"Front-normal velocity (m/day)\\n(red = retreat, blue = advance)\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421d6b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "windows = [\n",
    "    (\"2019-04-01\", \"2019-05-01\"),\n",
    "    (\"2019-05-01\", \"2019-06-01\"),\n",
    "    (\"2019-06-01\", \"2019-07-01\"),\n",
    "]\n",
    "\n",
    "v_list, titles = [], []\n",
    "for t1, t2 in windows:\n",
    "    ds = front_normal_velocity(da, t1, t2, front_value=0.5, bandwidth=0.05, smooth_px=1)\n",
    "    v_list.append(ds[\"v_normal\"])\n",
    "    titles.append(f\"{ds.attrs['t1']} → {ds.attrs['t2']}\")\n",
    "\n",
    "# shared, robust, symmetric limits\n",
    "all_vals = np.concatenate([v.values.ravel() for v in v_list])\n",
    "all_vals = all_vals[np.isfinite(all_vals)]\n",
    "lim = np.quantile(np.abs(all_vals), 0.98)\n",
    "norm = mcolors.TwoSlopeNorm(vmin=-lim, vcenter=0.0, vmax=lim)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharex=True, sharey=True, constrained_layout=True)\n",
    "\n",
    "for ax, v, title in zip(axes, v_list, titles):\n",
    "    im = v.plot(ax=ax, cmap=\"RdBu_r\", norm=norm, add_colorbar=False)\n",
    "    ax.set_title(f\"Front-normal speed (m/day)\\n{title}\")\n",
    "    ax.set_xlabel(\"lon\")\n",
    "\n",
    "axes[0].set_ylabel(\"lat\")\n",
    "for ax in axes[1:]:\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "cbar = fig.colorbar(im, ax=axes, pad=0.02, shrink=0.9)\n",
    "cbar.set_label(\"front-normal velocity (m/day)\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab40265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example usage (adjust dates and front_value to your variable) ---\n",
    "ds_perp = front_normal_velocity(da, \"2019-05-15\", \"2019-06-01\", front_value=0.5, bandwidth=0.05, smooth_px=1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "ds_perp[\"v_normal\"].plot(\n",
    "    ax=ax,\n",
    "    cmap=\"RdBu_r\",\n",
    "    robust=True,\n",
    "    cbar_kwargs={\"label\": \"front-normal velocity (m/day)\"}\n",
    ")\n",
    "ax.set_title(f\"Front-normal velocity\\n{ds_perp.attrs['t1']} → {ds_perp.attrs['t2']}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116e78d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c02e734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example usage (adjust dates and front_value to your variable) ---\n",
    "ds_perp = front_normal_velocity(da, \"2019-05-15\", \"2019-06-01\", front_value=0.5, bandwidth=0.05, smooth_px=1)\n",
    "ds_perp[\"v_normal\"].plot(cmap=\"RdBu_r\", robust=True)  # m/day; red/blue = retreat/advance\n",
    "# ds_perp[\"mask_front\"].plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704751be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regionmask\n",
    "from shapely.ops import unary_union\n",
    "\n",
    "# --- Load basin regions ------------------------------------------------------\n",
    "gdf_split = gpd.read_file(\"okavango_regions.gpkg\")\n",
    "# ensure a basin_label column exists\n",
    "if \"basin_label\" not in gdf_split.columns:\n",
    "    gdf_split[\"basin_label\"] = gdf_split[\"name\"]\n",
    "\n",
    "# --- Alias watermask & derive spatial extent from da -------------------------\n",
    "wm = da\n",
    "lon_min = float(da[lon_name].min());  lon_max = float(da[lon_name].max())\n",
    "lat_min = float(da[lat_name].min());  lat_max = float(da[lat_name].max())\n",
    "\n",
    "# --- Study area polygon (union of all basin geometries) ----------------------\n",
    "study_area = unary_union(gdf_split.geometry)\n",
    "\n",
    "# --- Config ------------------------------------------------------------------\n",
    "topN     = 5     # label the N basins with highest mean inundated area\n",
    "map_time = None  # set to e.g. \"2020-06-01\" to pick a specific time slice\n",
    "\n",
    "# --- Time series: mean water fraction per basin per time step ----------------\n",
    "regions  = regionmask.from_geopandas(gdf_split, names=\"basin_label\")\n",
    "mask3d   = regions.mask_3D(da[lon_name].values, da[lat_name].values)  # (region, lat, lon)\n",
    "\n",
    "# Iterate over the actual region indices regionmask assigned (may skip dupes)\n",
    "region_nums = mask3d.region.values  # integer indices into gdf_split\n",
    "ts_dict = {}\n",
    "for reg_i in region_nums:\n",
    "    label = gdf_split.iloc[int(reg_i)][\"basin_label\"]\n",
    "    basin_mask = mask3d.sel(region=reg_i)\n",
    "    ts_dict[label] = da.where(basin_mask).mean(dim=[lat_name, lon_name]).values\n",
    "\n",
    "ts = pd.DataFrame(ts_dict, index=pd.DatetimeIndex(da[\"time\"].values))\n",
    "print(f\"ts shape: {ts.shape}  |  basins: {list(ts.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1a44e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as pe\n",
    "\n",
    "gdf_regions = gpd.read_file(\"okavango_regions.gpkg\")\n",
    "print(gdf_regions[[\"name\", \"geometry\"]].to_string())\n",
    "print(f\"\\nCRS: {gdf_regions.crs} | rows: {len(gdf_regions)}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "gdf_regions.plot(ax=ax, facecolor=\"lightblue\", edgecolor=\"black\", linewidth=0.8, alpha=0.5)\n",
    "\n",
    "for _, row in gdf_regions.iterrows():\n",
    "    p = row.geometry.centroid\n",
    "    if not row.geometry.contains(p):\n",
    "        p = row.geometry.representative_point()\n",
    "    ax.text(\n",
    "        p.x, p.y, row[\"name\"],\n",
    "        ha=\"center\", va=\"center\", fontsize=8,\n",
    "        path_effects=[pe.withStroke(linewidth=2, foreground=\"white\")]\n",
    "    )\n",
    "\n",
    "ax.set_title(\"okavango_regions.gpkg\")\n",
    "ax.set_xlabel(\"Longitude\")\n",
    "ax.set_ylabel(\"Latitude\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f9d6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Map plot: pick a single 2D time slice + labels clipped to data extent\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.patheffects as pe\n",
    "from shapely.geometry import box\n",
    "\n",
    "# ---- choose which basins to label (topN by mean inundated area) ----\n",
    "top_cols = ts.mean().sort_values(ascending=False).head(topN).index.tolist()\n",
    "\n",
    "# ---- pick a SINGLE time slice robustly (always returns 2D lat×lon) ----\n",
    "time_index = pd.DatetimeIndex(wm[\"time\"].values)\n",
    "\n",
    "if map_time is None:\n",
    "    tidx = -1\n",
    "else:\n",
    "    tidx = time_index.get_indexer([pd.Timestamp(map_time)], method=\"nearest\")[0]\n",
    "\n",
    "wm_map = wm.isel(time=tidx)\n",
    "tsel = time_index[tidx]\n",
    "\n",
    "extent_box = box(lon_min, lat_min, lon_max, lat_max)\n",
    "\n",
    "# ---- plot ----\n",
    "fig, ax = plt.subplots(figsize=(9, 6), subplot_kw={\"projection\": ccrs.PlateCarree()})\n",
    "\n",
    "wm_map.plot.pcolormesh(\n",
    "    ax=ax,\n",
    "    x=lon_name, y=lat_name,\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    add_colorbar=False,\n",
    "    cmap=\"Blues\",\n",
    ")\n",
    "\n",
    "ax.set_extent([lon_min, lon_max, lat_min, lat_max], crs=ccrs.PlateCarree())\n",
    "ax.add_feature(cfeature.LAND, zorder=0, facecolor=\"none\")\n",
    "ax.add_feature(cfeature.BORDERS, linewidth=0.3)\n",
    "ax.coastlines(linewidth=0.3)\n",
    "\n",
    "# ---- draw watershed delineations ----\n",
    "for _, row in gdf_split.iterrows():\n",
    "    ax.add_geometries(\n",
    "        [row.geometry],\n",
    "        crs=ccrs.PlateCarree(),\n",
    "        facecolor=\"none\",\n",
    "        edgecolor=\"gold\",\n",
    "        linewidth=1.0,\n",
    "        zorder=5,\n",
    "    )\n",
    "\n",
    "# ---- labels: all top basins ----\n",
    "for _, r in gdf_split[gdf_split[\"basin_label\"].isin(top_cols)].iterrows():\n",
    "    g = r.geometry.intersection(extent_box)\n",
    "    if g.is_empty:\n",
    "        continue\n",
    "    p = g.centroid\n",
    "    if not g.contains(p):\n",
    "        p = g.representative_point()\n",
    "    ax.text(\n",
    "        p.x, p.y, r[\"basin_label\"],\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        ha=\"center\", va=\"center\",\n",
    "        fontsize=8,\n",
    "        zorder=10,\n",
    "        path_effects=[pe.withStroke(linewidth=2, foreground=\"white\")]\n",
    "    )\n",
    "\n",
    "ax.set_title(f\"CYGNSS watermask + basins — {pd.Timestamp(tsel).strftime('%Y-%m')}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65d9ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the first time slice\n",
    "da0 = da.isel(time=10)\n",
    "# Optional: quick stats in the console\n",
    "t0 = pd.to_datetime(da0.time.values).strftime(\"%Y-%m-%d %H:%M:%S\") if \"time\" in da0.coords else \"unknown time\"\n",
    "vmin = np.nanmin(da0.values)\n",
    "vmax = np.nanmax(da0.values)\n",
    "print(f\"First image time: {t0} | min={vmin:.3g}, max={vmax:.3g}\")\n",
    "# Plot (uses lat/lon coords if present)\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "da0.plot.pcolormesh(ax=ax, shading=\"auto\", cmap=\"Blues\", add_colorbar=False)  # robust to ascending/descending lat\n",
    "ax.set_title(f\"{da0.name or 'variable'} at {t0}\")\n",
    "ax.set_aspect(\"equal\")  # feel free to comment out if you prefer auto aspect\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99a5110",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7e803e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config -------------------------------------------------------------------\n",
    "PLOT_ASPECT_EQUAL = True\n",
    "CBAR_KW = {\"shrink\": 0.7, \"aspect\": 30, \"pad\": 0.02}\n",
    "# --- Choose the two times -----------------------------------------------------\n",
    "# Option A: by integer index\n",
    "t1_idx, t2_idx = 0, 31\n",
    "# Option B: by datetime-like labels (uncomment if you prefer)\n",
    "# t1_lbl, t2_lbl = \"2019-07-01\", \"2020-07-01\"\n",
    "# --- Grab slices (works whether you use indices or labels) --------------------\n",
    "if \"time\" not in da.dims:\n",
    "    raise ValueError(\"This DataArray has no 'time' dimension.\")\n",
    "# Index-based\n",
    "da1 = da.isel(time=t1_idx)\n",
    "da2 = da.isel(time=t2_idx)\n",
    "# Label-based (override above if using labels)\n",
    "# da1 = da.sel(time=np.datetime64(t1_lbl))\n",
    "# da2 = da.sel(time=np.datetime64(t2_lbl))\n",
    "# Date-only strings for titles/labels\n",
    "t1_dt = pd.to_datetime(da1.time.values)\n",
    "t2_dt = pd.to_datetime(da2.time.values)\n",
    "t1 = t1_dt.strftime(\"%Y-%m-%d\")\n",
    "t2 = t2_dt.strftime(\"%Y-%m-%d\")\n",
    "# --- Quick sanity stats for each slice ---------------------------------------\n",
    "stats(f\"Slice @ {t1}\", da1)\n",
    "stats(f\"Slice @ {t2}\", da2)\n",
    "# --- Difference (t2 - t1) ----------------------------------------------------\n",
    "diff = da2 - da1\n",
    "diff.name = (da.name or \"variable\") + \"_diff\"\n",
    "v = np.asarray(diff.values)\n",
    "print(f\"Diff ({t2} - {t1}): min={np.nanmin(v):.3g}, max={np.nanmax(v):.3g}, mean={np.nanmean(v):.3g}\")\n",
    "# --- Plot the two slices + difference ----------------------------------------\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import TwoSlopeNorm, LinearSegmentedColormap\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), constrained_layout=True,\n",
    "                         sharex=True, sharey=True)\n",
    "da1.plot.pcolormesh(ax=axes[0], shading=\"auto\", cmap=\"Blues\", add_colorbar=False)\n",
    "axes[0].set_title(t1)\n",
    "if PLOT_ASPECT_EQUAL:\n",
    "    axes[0].set_aspect(\"equal\")\n",
    "da2.plot.pcolormesh(ax=axes[1], shading=\"auto\", cmap=\"Blues\", add_colorbar=False)\n",
    "axes[1].set_title(t2)\n",
    "if PLOT_ASPECT_EQUAL:\n",
    "    axes[1].set_aspect(\"equal\")\n",
    "# Mask zeros so they render as transparent (white background shows through)\n",
    "# Custom diverging: tan (-1, loss) → white (0) → steel blue (+1, expansion)\n",
    "LOSS_COLOR = \"#C4A882\"   # tan → water loss\n",
    "GAIN_COLOR = \"#4A7BA1\"   # steel blue → water expansion\n",
    "cmap_custom = LinearSegmentedColormap.from_list(\n",
    "    \"tan_steelblue\", [LOSS_COLOR, \"#FFFFFF\", GAIN_COLOR]\n",
    ")\n",
    "diff_masked = diff.where(diff != 0)\n",
    "diff_norm = TwoSlopeNorm(vmin=-1, vcenter=0, vmax=1)\n",
    "diff_masked.plot.pcolormesh(\n",
    "    ax=axes[2],\n",
    "    shading=\"auto\",\n",
    "    cmap=cmap_custom,\n",
    "    norm=diff_norm,\n",
    "    add_colorbar=False,\n",
    ")\n",
    "axes[2].set_title(f\"Δ ({t2} − {t1})\")\n",
    "if PLOT_ASPECT_EQUAL:\n",
    "    axes[2].set_aspect(\"equal\")\n",
    "# Legend colors match the custom colormap endpoints\n",
    "loss_patch = mpatches.Patch(color=LOSS_COLOR, label=\"water loss\")\n",
    "gain_patch = mpatches.Patch(color=GAIN_COLOR, label=\"water expansion\")\n",
    "axes[2].legend(handles=[loss_patch, gain_patch], loc=\"lower left\", fontsize=12, framealpha=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17a2f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import TwoSlopeNorm, LinearSegmentedColormap\n",
    "\n",
    "def _edge_mean(da, t, days=10, side=\"after\"):\n",
    "    t = pd.Timestamp(t)\n",
    "    if side == \"after\":\n",
    "        sl = slice(t, t + pd.Timedelta(days=days - 1))\n",
    "    else:  # \"before\"\n",
    "        sl = slice(t - pd.Timedelta(days=days - 1), t)\n",
    "    return da.sel(time=sl).mean(\"time\", skipna=True)\n",
    "\n",
    "def expansion_contraction(da, t1, t2, front_value=0.5, edge_days=10):\n",
    "    early = _edge_mean(da, t1, days=edge_days, side=\"after\")\n",
    "    late  = _edge_mean(da, t2, days=edge_days, side=\"before\")\n",
    "    return (late >= front_value).astype(\"i1\") - (early >= front_value).astype(\"i1\")\n",
    "\n",
    "windows = [\n",
    "    (\"2019-01-01\", \"2019-02-01\", \"2019 Jan–Feb\"),\n",
    "    (\"2020-01-01\", \"2020-02-01\", \"2020 Jan–Feb\"),\n",
    "    (\"2021-01-01\", \"2021-02-01\", \"2021 Jan–Feb\"),\n",
    "]\n",
    "\n",
    "deltas = [expansion_contraction(da, t1, t2, front_value=0.5, edge_days=10)\n",
    "          for t1, t2, _ in windows]\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    1, 3, figsize=(15, 5),\n",
    "    sharex=True, sharey=True,\n",
    "    constrained_layout=True\n",
    ")\n",
    "\n",
    "# Custom diverging: tan (-1, loss) → white (0) → steel blue (+1, expansion)\n",
    "LOSS_COLOR = \"#C4A882\"   # tan → water loss\n",
    "GAIN_COLOR = \"#4A7BA1\"   # steel blue → water expansion\n",
    "cmap_custom = LinearSegmentedColormap.from_list(\n",
    "    \"tan_steelblue\", [LOSS_COLOR, \"#FFFFFF\", GAIN_COLOR]\n",
    ")\n",
    "norm = TwoSlopeNorm(vmin=-1, vcenter=0, vmax=1)\n",
    "\n",
    "for ax, d, (_, _, title) in zip(axes, deltas, windows):\n",
    "    # Mask zeros so they render as transparent (white background shows through)\n",
    "    d_masked = d.where(d != 0)\n",
    "    d_masked.plot(ax=ax, cmap=cmap_custom, norm=norm, add_colorbar=False)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"lon\")\n",
    "\n",
    "axes[0].set_ylabel(\"lat\")\n",
    "for ax in axes[1:]:\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "# Legend matches the custom colormap endpoints\n",
    "loss_patch = mpatches.Patch(color=LOSS_COLOR, label=\"water loss\")\n",
    "gain_patch = mpatches.Patch(color=GAIN_COLOR, label=\"water expansion\")\n",
    "axes[-1].legend(handles=[loss_patch, gain_patch], loc=\"lower left\", fontsize=12, framealpha=0.8)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d016140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config -------------------------------------------------------------------\n",
    "PLOT_ASPECT_EQUAL = True\n",
    "CBAR_KW = {\"shrink\": 0.75, \"aspect\": 30, \"pad\": 0.02}\n",
    "# --- Choose the two times -----------------------------------------------------\n",
    "# Option A: by integer index\n",
    "t1_idx, t2_idx = 3, 10\n",
    "# Option B: by datetime-like labels (uncomment if you prefer)\n",
    "# t1_lbl, t2_lbl = \"2019-07-01\", \"2020-07-01\"\n",
    "# --- Grab slices (works whether you use indices or labels) --------------------\n",
    "if \"time\" not in da.dims:\n",
    "    raise ValueError(\"This DataArray has no 'time' dimension.\")\n",
    "# Index-based\n",
    "da1 = da.isel(time=t1_idx)\n",
    "da2 = da.isel(time=t2_idx)\n",
    "# Label-based (override above if using labels)\n",
    "# da1 = da.sel(time=np.datetime64(t1_lbl))\n",
    "# da2 = da.sel(time=np.datetime64(t2_lbl))\n",
    "# Date-only strings for titles/labels\n",
    "t1_dt = pd.to_datetime(da1.time.values)\n",
    "t2_dt = pd.to_datetime(da2.time.values)\n",
    "t1 = t1_dt.strftime(\"%Y-%m-%d\")\n",
    "t2 = t2_dt.strftime(\"%Y-%m-%d\")\n",
    "# --- Quick sanity stats for each slice ---------------------------------------\n",
    "stats(f\"Slice @ {t1}\", da1)\n",
    "stats(f\"Slice @ {t2}\", da2)\n",
    "# --- Difference (t2 - t1) ----------------------------------------------------\n",
    "diff = da2 - da1\n",
    "diff.name = (da.name or \"variable\") + \"_diff\"\n",
    "v = np.asarray(diff.values)\n",
    "print(f\"Diff ({t2} - {t1}): min={np.nanmin(v):.3g}, max={np.nanmax(v):.3g}, mean={np.nanmean(v):.3g}\")\n",
    "# --- Plot the two slices + difference ----------------------------------------\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import TwoSlopeNorm, LinearSegmentedColormap\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4.5), constrained_layout=True,\n",
    "                         sharex=True, sharey=True)\n",
    "da1.plot.pcolormesh(ax=axes[0], shading=\"auto\", cmap=\"Blues\", add_colorbar=False)\n",
    "axes[0].set_title(t1)\n",
    "if PLOT_ASPECT_EQUAL:\n",
    "    axes[0].set_aspect(\"equal\")\n",
    "da2.plot.pcolormesh(ax=axes[1], shading=\"auto\", cmap=\"Blues\", add_colorbar=False)\n",
    "axes[1].set_title(t2)\n",
    "if PLOT_ASPECT_EQUAL:\n",
    "    axes[1].set_aspect(\"equal\")\n",
    "# Mask zeros so they render as transparent (white background shows through)\n",
    "# Custom diverging: tan (-1, loss) → white (0) → steel blue (+1, expansion)\n",
    "LOSS_COLOR = \"#C4A882\"   # tan → water loss\n",
    "GAIN_COLOR = \"#4A7BA1\"   # steel blue → water expansion\n",
    "cmap_custom = LinearSegmentedColormap.from_list(\n",
    "    \"tan_steelblue\", [LOSS_COLOR, \"#FFFFFF\", GAIN_COLOR]\n",
    ")\n",
    "diff_masked = diff.where(diff != 0)\n",
    "diff_norm = TwoSlopeNorm(vmin=-1, vcenter=0, vmax=1)\n",
    "diff_masked.plot.pcolormesh(\n",
    "    ax=axes[2],\n",
    "    shading=\"auto\",\n",
    "    cmap=cmap_custom,\n",
    "    norm=diff_norm,\n",
    "    add_colorbar=False,\n",
    ")\n",
    "axes[2].set_title(f\"Δ ({t2} − {t1})\")\n",
    "if PLOT_ASPECT_EQUAL:\n",
    "    axes[2].set_aspect(\"equal\")\n",
    "# Legend colors match the custom colormap endpoints\n",
    "loss_patch = mpatches.Patch(color=LOSS_COLOR, label=\"water loss\")\n",
    "gain_patch = mpatches.Patch(color=GAIN_COLOR, label=\"water expansion\")\n",
    "axes[2].legend(handles=[loss_patch, gain_patch], loc=\"lower left\", fontsize=12, framealpha=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4506dbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ── Compute FNV for every consecutive month pair, then composite by calendar month ──\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap, ListedColormap\n",
    "from matplotlib.patches import Patch\n",
    "import calendar\n",
    "from pathlib import Path\n",
    "\n",
    "# ── Cache paths ───────────────────────────────────────────────────────────────\n",
    "PROCESSED_DIR = Path(INPUT_DIR).parent / \"processed\"\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_EXP = PROCESSED_DIR / \"fnv_monthly_expansion.nc\"\n",
    "CACHE_CON = PROCESSED_DIR / \"fnv_monthly_contraction.nc\"\n",
    "\n",
    "FORCE_RECOMPUTE = False   # set True to ignore cache and recompute\n",
    "\n",
    "MONTH_NAMES = [calendar.month_abbr[m] for m in range(1, 13)]\n",
    "\n",
    "# ── Load from cache if available ──────────────────────────────────────────────\n",
    "if not FORCE_RECOMPUTE and CACHE_EXP.exists() and CACHE_CON.exists():\n",
    "    print(f\"Loading cached results from {PROCESSED_DIR}\")\n",
    "    v_stack_monthly_exp = xr.open_dataarray(CACHE_EXP)\n",
    "    v_stack_monthly_con = xr.open_dataarray(CACHE_CON)\n",
    "    months_available = v_stack_monthly_exp.calendar_month.values.tolist()\n",
    "    expansion_monthly   = {m: v_stack_monthly_exp.sel(calendar_month=m).drop_vars(\"calendar_month\")\n",
    "                           for m in months_available}\n",
    "    contraction_monthly = {m: v_stack_monthly_con.sel(calendar_month=m).drop_vars(\"calendar_month\")\n",
    "                           for m in months_available}\n",
    "    print(f\"Loaded composites for months: {months_available}\")\n",
    "\n",
    "else:\n",
    "    # ── Compute from scratch ──────────────────────────────────────────────────\n",
    "    time_index = pd.DatetimeIndex(da.time.values)\n",
    "    month_starts = (\n",
    "        pd.period_range(time_index.min(), time_index.max(), freq=\"M\")\n",
    "        .to_timestamp()\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    v_maps = []     # list of (calendar_month_int, DataArray)\n",
    "    skipped = 0\n",
    "    for i in range(len(month_starts) - 1):\n",
    "        t1 = month_starts[i].strftime(\"%Y-%m-%d\")\n",
    "        t2 = month_starts[i + 1].strftime(\"%Y-%m-%d\")\n",
    "        t1_data = da.sel(time=slice(t1, (month_starts[i + 1] - pd.Timedelta(days=1)).strftime(\"%Y-%m-%d\")))\n",
    "        t2_data = da.sel(time=slice(t2, (month_starts[i + 1] + pd.Timedelta(days=30)).strftime(\"%Y-%m-%d\")))\n",
    "        if t1_data.sizes[\"time\"] == 0 or t2_data.sizes[\"time\"] == 0:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        try:\n",
    "            ds_v = front_normal_velocity(da, t1, t2, front_value=0.5, bandwidth=0.05, smooth_px=1)\n",
    "            v_maps.append((month_starts[i].month, ds_v[\"v_normal\"]))\n",
    "        except Exception:\n",
    "            skipped += 1\n",
    "\n",
    "    print(f\"Computed FNV for {len(v_maps)} month pairs ({skipped} skipped)\")\n",
    "\n",
    "    # ── Group by calendar month ───────────────────────────────────────────────\n",
    "    expansion_monthly   = {}\n",
    "    contraction_monthly = {}\n",
    "    for cal_month in range(1, 13):\n",
    "        maps_this_month = [v for (m, v) in v_maps if m == cal_month]\n",
    "        if not maps_this_month:\n",
    "            continue\n",
    "        stack = xr.concat(maps_this_month, dim=\"year\")\n",
    "        expansion_monthly[cal_month]   = stack.where(stack > 0).mean(\"year\", skipna=True)\n",
    "        contraction_monthly[cal_month] = (-stack).where(stack < 0).mean(\"year\", skipna=True)\n",
    "\n",
    "    months_available = sorted(expansion_monthly.keys())\n",
    "    v_stack_monthly_exp = xr.concat(\n",
    "        [expansion_monthly[m]   for m in months_available],\n",
    "        dim=pd.Index(months_available, name=\"calendar_month\")\n",
    "    )\n",
    "    v_stack_monthly_con = xr.concat(\n",
    "        [contraction_monthly[m] for m in months_available],\n",
    "        dim=pd.Index(months_available, name=\"calendar_month\")\n",
    "    )\n",
    "\n",
    "    # ── Save to cache ─────────────────────────────────────────────────────────\n",
    "    v_stack_monthly_exp.to_netcdf(CACHE_EXP)\n",
    "    v_stack_monthly_con.to_netcdf(CACHE_CON)\n",
    "    print(f\"Saved results to {PROCESSED_DIR}\")\n",
    "\n",
    "# ── Expose all-month means for downstream cells ───────────────────────────────\n",
    "expansion_mean   = v_stack_monthly_exp.mean(\"calendar_month\", skipna=True)\n",
    "contraction_mean = v_stack_monthly_con.mean(\"calendar_month\", skipna=True)\n",
    "\n",
    "# ── Colour limits (shared across months) ─────────────────────────────────────\n",
    "EXPAND_COLOR   = \"#4A7BA1\"\n",
    "CONTRACT_COLOR = \"#C4A882\"\n",
    "cmap_expand   = LinearSegmentedColormap.from_list(\"expand\",   [\"#FFFFFF\", EXPAND_COLOR])\n",
    "cmap_contract = LinearSegmentedColormap.from_list(\"contract\", [\"#FFFFFF\", CONTRACT_COLOR])\n",
    "\n",
    "exp_all = np.concatenate([\n",
    "    expansion_monthly[m].values[np.isfinite(expansion_monthly[m].values)]\n",
    "    for m in months_available\n",
    "])\n",
    "con_all = np.concatenate([\n",
    "    contraction_monthly[m].values[np.isfinite(contraction_monthly[m].values)]\n",
    "    for m in months_available\n",
    "])\n",
    "exp_vmax = float(np.nanpercentile(exp_all, 95))\n",
    "con_vmax = float(np.nanpercentile(con_all, 95))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ef419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Plot a single calendar month ──────────────────────────────────────────────\n",
    "PLOT_MONTH = 10   # May\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5), constrained_layout=True)\n",
    "\n",
    "expansion_monthly[PLOT_MONTH].plot(\n",
    "    ax=axes[0], cmap=cmap_expand, vmin=0, vmax=exp_vmax, add_colorbar=True,\n",
    "    cbar_kwargs={\"label\": \"mean expansion speed (m/day)\", \"shrink\": 0.8}\n",
    ")\n",
    "axes[0].set_title(f\"Mean flood expansion speed — {MONTH_NAMES[PLOT_MONTH - 1]}\")\n",
    "axes[0].set_aspect(\"equal\")\n",
    "\n",
    "contraction_monthly[PLOT_MONTH].plot(\n",
    "    ax=axes[1], cmap=cmap_contract, vmin=0, vmax=con_vmax, add_colorbar=True,\n",
    "    cbar_kwargs={\"label\": \"mean contraction speed (m/day)\", \"shrink\": 0.8}\n",
    ")\n",
    "axes[1].set_title(f\"Mean flood contraction speed — {MONTH_NAMES[PLOT_MONTH - 1]}\")\n",
    "axes[1].set_aspect(\"equal\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341205d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ── Re-plot with outliers removed (clip at 2nd–98th percentile per map) ──────\n",
    "from matplotlib.colors import LinearSegmentedColormap, ListedColormap, TwoSlopeNorm\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "EXPAND_COLOR   = \"#4A7BA1\"\n",
    "CONTRACT_COLOR = \"#C4A882\"\n",
    "PERM_COLOR     = \"#0D1B2A\"   # dark navy/blackish for permanently inundated\n",
    "\n",
    "cmap_expand   = LinearSegmentedColormap.from_list(\"expand\",   [\"#FFFFFF\", EXPAND_COLOR])\n",
    "cmap_contract = LinearSegmentedColormap.from_list(\"contract\", [\"#FFFFFF\", CONTRACT_COLOR])\n",
    "cmap_diff     = LinearSegmentedColormap.from_list(\"diff\",     [CONTRACT_COLOR, \"#FFFFFF\", EXPAND_COLOR])\n",
    "cmap_perm     = ListedColormap([PERM_COLOR])\n",
    "\n",
    "def clip_outliers(da, lo=0, hi=95):\n",
    "    \"\"\"Return da with values outside [lo, hi] percentile clamped to the threshold.\"\"\"\n",
    "    vals = da.values\n",
    "    finite = vals[np.isfinite(vals)]\n",
    "    vmin, vmax = np.nanpercentile(finite, lo), np.nanpercentile(finite, hi)\n",
    "    return da.clip(min=vmin, max=vmax)\n",
    "\n",
    "expansion_clipped   = clip_outliers(expansion_mean)\n",
    "contraction_clipped = clip_outliers(contraction_mean)\n",
    "\n",
    "# expansion − contraction: positive = expansion faster, negative = contraction faster\n",
    "speed_diff = expansion_clipped - contraction_clipped\n",
    "\n",
    "# Pixels inundated >90% of all time steps → permanently wet\n",
    "perm_inund = (da > 0.5).mean(\"time\") > 0.9\n",
    "perm_overlay = perm_inund.where(perm_inund).astype(float)\n",
    "\n",
    "# Colour limits\n",
    "exp_vmax  = float(np.nanmax(expansion_clipped.values[np.isfinite(expansion_clipped.values)]))\n",
    "con_vmax  = float(np.nanmax(contraction_clipped.values[np.isfinite(contraction_clipped.values)]))\n",
    "diff_vals = speed_diff.values[np.isfinite(speed_diff.values)]\n",
    "diff_lim  = float(np.nanpercentile(np.abs(diff_vals), 95))\n",
    "diff_norm = TwoSlopeNorm(vmin=-diff_lim, vcenter=0, vmax=diff_lim)\n",
    "\n",
    "perm_legend = [Patch(facecolor=PERM_COLOR, label=\"Permanently inundated (>90%)\")]\n",
    "\n",
    "# ── Figure 1: expansion + contraction ────────────────────────────────────────\n",
    "fig1, axes1 = plt.subplots(1, 2, figsize=(12, 5), constrained_layout=True)\n",
    "\n",
    "expansion_clipped.plot(\n",
    "    ax=axes1[0], cmap=cmap_expand, vmin=0, vmax=exp_vmax, add_colorbar=True,\n",
    "    cbar_kwargs={\"label\": \"mean expansion speed (m/day)\", \"shrink\": 0.8}\n",
    ")\n",
    "perm_overlay.plot(ax=axes1[0], cmap=cmap_perm, vmin=0.5, vmax=1.5, add_colorbar=False)\n",
    "axes1[0].set_title(\"Mean flood expansion speed\")\n",
    "axes1[0].set_aspect(\"equal\")\n",
    "axes1[0].legend(handles=perm_legend, loc=\"lower left\", fontsize=8)\n",
    "\n",
    "contraction_clipped.plot(\n",
    "    ax=axes1[1], cmap=cmap_contract, vmin=0, vmax=con_vmax, add_colorbar=True,\n",
    "    cbar_kwargs={\"label\": \"mean contraction speed (m/day)\", \"shrink\": 0.8}\n",
    ")\n",
    "perm_overlay.plot(ax=axes1[1], cmap=cmap_perm, vmin=0.5, vmax=1.5, add_colorbar=False)\n",
    "axes1[1].set_title(\"Mean flood contraction speed\")\n",
    "axes1[1].set_aspect(\"equal\")\n",
    "axes1[1].legend(handles=perm_legend, loc=\"lower left\", fontsize=8)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ── Figure 2: expansion − contraction difference ─────────────────────────────\n",
    "fig2, ax2 = plt.subplots(1, 1, figsize=(7, 5), constrained_layout=True)\n",
    "\n",
    "speed_diff.plot(\n",
    "    ax=ax2, cmap=cmap_diff, norm=diff_norm, add_colorbar=True,\n",
    "    cbar_kwargs={\"label\": \"expansion − contraction (m/day)\", \"shrink\": 0.8}\n",
    ")\n",
    "perm_overlay.plot(ax=ax2, cmap=cmap_perm, vmin=0.5, vmax=1.5, add_colorbar=False)\n",
    "ax2.set_title(\"Expansion − contraction speed\\n(blue = expansion faster, tan = contraction faster)\")\n",
    "ax2.set_aspect(\"equal\")\n",
    "ax2.legend(handles=perm_legend, loc=\"lower left\", fontsize=8)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4868cdba",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f617120",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256f320f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01458afd",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Utilities\n",
    "# -------------------------------\n",
    "# -------------------------------\n",
    "# Quick plotting helpers\n",
    "# -------------------------------\n",
    "# def plot_month_grid(da_monthly_diff: xr.DataArray, title: str = \"Monthly composite of differences (m2 - m1)\"):\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     from matplotlib.colors import TwoSlopeNorm\n",
    "#     import numpy as np, calendar\n",
    "#     # robust symmetric scaling across all months\n",
    "#     v = da_monthly_diff.values\n",
    "#     vmax = np.nanpercentile(np.abs(v), 98) if np.isfinite(v).any() else 1.0\n",
    "#     norm = TwoSlopeNorm(vmin=-vmax, vcenter=0.0, vmax=vmax)\n",
    "#     fig, axes = plt.subplots(3, 4, figsize=(14, 9), constrained_layout=True)\n",
    "#     last_im = None\n",
    "#     for m, ax in enumerate(axes.ravel(), start=1):\n",
    "#         if \"month\" in da_monthly_diff.dims:\n",
    "#             arr = da_monthly_diff.sel(month=m)\n",
    "#             last_im = arr.plot(ax=ax, cmap=\"RdBu_r\", norm=norm, add_colorbar=False)\n",
    "#             ax.set_title(calendar.month_abbr[m])\n",
    "#         else:\n",
    "#             # single composite (no month dim)\n",
    "#             last_im = da_monthly_diff.plot(ax=ax, cmap=\"RdBu_r\", norm=norm, add_colorbar=False)\n",
    "#             ax.set_title(title)\n",
    "#             break\n",
    "#     if last_im is not None:\n",
    "#         cbar = fig.colorbar(last_im, ax=axes.ravel().tolist(), fraction=0.025, pad=0.01)\n",
    "#         cbar.set_label(\"Difference (units of variable)\")\n",
    "#     plt.suptitle(title, y=1.02, fontsize=12)\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21394319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt = da.time.diff(\"time\") / np.timedelta64(1, \"D\")\n",
    "# rate = da.diff(\"time\") / dt\n",
    "# rate = rate.assign_coords(time=da.time.isel(time=slice(1, None)))\n",
    "# rate_monthly = rate.groupby(\"time.month\").mean(\"time\")\n",
    "# plot_month_grid(rate_monthly, title=\"Monthly composite of rate (per day)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4e4477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca7dc19f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45829ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your existing picks\n",
    "da1 = da.isel(time=t1_idx)\n",
    "da2 = da.isel(time=t2_idx)\n",
    "t1_dt = pd.to_datetime(da1.time.values)\n",
    "t2_dt = pd.to_datetime(da2.time.values)\n",
    "t1 = t1_dt.strftime(\"%Y-%m-%d\"); t2 = t2_dt.strftime(\"%Y-%m-%d\")\n",
    "# your three‑panel quick look:\n",
    "diff = da2 - da1\n",
    "plot_three_panel(da1, da2, diff, t1, t2, aspect_equal=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859a5fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "\n",
    "def _edge_mean(da, t, days=10, side=\"after\"):\n",
    "    t = pd.Timestamp(t)\n",
    "    if side == \"after\":\n",
    "        sl = slice(t, t + pd.Timedelta(days=days - 1))\n",
    "    else:  # \"before\"\n",
    "        sl = slice(t - pd.Timedelta(days=days - 1), t)\n",
    "    return da.sel(time=sl).mean(\"time\", skipna=True)\n",
    "\n",
    "def expansion_contraction(da, t1, t2, front_value=0.5, edge_days=10):\n",
    "    early = _edge_mean(da, t1, days=edge_days, side=\"after\")\n",
    "    late  = _edge_mean(da, t2, days=edge_days, side=\"before\")\n",
    "    return (late >= front_value).astype(\"i1\") - (early >= front_value).astype(\"i1\")\n",
    "\n",
    "windows = [\n",
    "    (\"2019-01-01\", \"2019-05-01\", \"2019 Jan–May\"),\n",
    "    (\"2020-01-01\", \"2020-05-01\", \"2020 Jan–May\"),\n",
    "    (\"2021-01-01\", \"2021-05-01\", \"2021 Jan–May\"),\n",
    "]\n",
    "\n",
    "deltas = [expansion_contraction(da, t1, t2, front_value=0.5, edge_days=10)\n",
    "          for t1, t2, _ in windows]\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    1, 3, figsize=(15, 5),\n",
    "    sharex=True, sharey=True,\n",
    "    constrained_layout=True\n",
    ")\n",
    "\n",
    "norm = TwoSlopeNorm(vmin=-1, vcenter=0, vmax=1)\n",
    "\n",
    "for ax, d, (_, _, title) in zip(axes, deltas, windows):\n",
    "    im = d.plot(ax=ax, cmap=\"RdBu_r\", norm=norm, add_colorbar=False)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"lon\")\n",
    "\n",
    "axes[0].set_ylabel(\"lat\")\n",
    "for ax in axes[1:]:\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "cbar = fig.colorbar(im, ax=axes, pad=0.02, shrink=0.9, ticks=[-1, 0, 1])\n",
    "cbar.set_ticklabels([\"contraction\", \"no change\", \"expansion\"])\n",
    "cbar.set_label(\"Net change in (da ≥ 0.5)\\n(last 10d of window − first 10d)\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686db675",
   "metadata": {
    "code_folding": [
     10
    ]
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import xarray as xr\n",
    "# import matplotlib.pyplot as plt\n",
    "# --- Config you can tweak -----------------------------------------------------\n",
    "WATER_THRESHOLD = None   # e.g., 0.5 for probability/fraction rasters; None = auto\n",
    "WATER_CLASSES   = {1,2,3,4}  # typical DSWE \"water\" classes\n",
    "PLOT_ASPECT_EQUAL = True\n",
    "# --- Build wet-mask across time and count \"extent\" ----------------------------\n",
    "wet = _infer_mask(da)\n",
    "# Count wet pixels at each time\n",
    "extent_ts = wet.sum(dim=(\"lat\", \"lon\"))\n",
    "# Identify index with minimum extent\n",
    "imin = int(extent_ts.argmin(dim=\"time\").values)\n",
    "t_min = pd.to_datetime(da.time.isel(time=imin).values).strftime(\"%Y-%m-%d %H:%M:%S\") if \"time\" in da.coords else \"unknown time\"\n",
    "pixels_min = int(extent_ts.isel(time=imin).values)\n",
    "print(f\"Smallest extent at time index {imin} ({t_min}) with {pixels_min} wet pixels.\")\n",
    "# --- Plot that slice ----------------------------------------------------------\n",
    "da_min = da.isel(time=imin)\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "da_min.plot.pcolormesh(ax=ax, shading=\"auto\")\n",
    "ax.set_title(f\"{da_min.name or 'variable'} — smallest extent @ {t_min}\\n(wet pixels = {pixels_min})\")\n",
    "if PLOT_ASPECT_EQUAL:\n",
    "    ax.set_aspect(\"equal\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db537d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example usage (adjust dates and front_value to your variable) ---\n",
    "ds_perp = front_normal_velocity(da, \"2019-01-15\", \"2019-06-15\", front_value=0.5, bandwidth=0.05, smooth_px=1)\n",
    "ds_perp[\"v_normal\"].plot(cmap=\"RdBu_r\", robust=True)  # m/day; red/blue = retreat/advance\n",
    "# ds_perp[\"mask_front\"].plot()\n",
    "# (Optional) overlay quiver of u_perp/v_perp on top of v_normal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2287bbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Quick look: June multi-year mean (m/day)\n",
    "# clim[\"v_normal_mean\"].sel(month=6).plot(cmap=\"RdBu_r\", robust=True)\n",
    "# plt.title(\"June multi-year mean front-normal speed (m/day)\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e044fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 12-panel overview (all months)\n",
    "# fig, axes = plt.subplots(3, 4, figsize=(14, 9), constrained_layout=True)\n",
    "# for m, ax in enumerate(axes.ravel(), start=1):\n",
    "#     im = clim[\"v_normal_mean\"].sel(month=m).plot(\n",
    "#         ax=ax, cmap=\"RdBu_r\", robust=True, add_colorbar=False\n",
    "#     )\n",
    "#     ax.set_title(calendar.month_abbr[m])\n",
    "# # shared colorbar\n",
    "# cbar = fig.colorbar(im, ax=axes.ravel().tolist(), fraction=0.025, pad=0.01)\n",
    "# cbar.set_label(\"Front-normal speed (m/day)\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9120952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example usage ---\n",
    "# 1) After you computed ds_perp with `front_normal_velocity(...)`:\n",
    "ds_perp = front_normal_velocity(da, \"2019-05-15\", \"2019-06-15\", front_value=0.5, bandwidth=0.05, smooth_px=1)\n",
    "# 2) Classify against SE only:\n",
    "cls = classify_front_direction(ds_perp, angle_tol_deg=30, min_speed_m_per_day=10.0)\n",
    "# 3) Or classify against radial \"outwards from apex\" (replace with your apex lat/lon):\n",
    "apex_ll = (-19.0, 22.5)  # <-- EXAMPLE ONLY, set your actual apex!\n",
    "cls = classify_front_direction(ds_perp, angle_tol_deg=30, min_speed_m_per_day=10.0, apex=apex_ll)\n",
    "# 4) Quick summaries:\n",
    "print(cls.attrs[\"summary\"])\n",
    "print(\"SE-positive share of front (strictly toward SE):\",\n",
    "      float(np.nansum((cls[\"mask_front\"]) & (cls[\"v_SE\"] > 0))) / float(np.nansum(cls[\"mask_front\"])) * 100, \"%\")\n",
    "# 1) Where is the front moving toward SE?\n",
    "plt.figure()\n",
    "cls[\"v_SE\"].plot(robust=True); plt.title(f\"Perp. motion along SE (m/day) — tol={cls.attrs['angle_tol_deg']}°\")\n",
    "# 3) If apex given: outward alignment\n",
    "if \"aligned_outward\" in cls:\n",
    "    plt.figure()\n",
    "    cls[\"aligned_outward\"].plot(); plt.title(\"Front pixels aligned with outward-from-apex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96217647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask to aligned pixels so we emphasize the front moving outward/inward\n",
    "signed_speed = cls[\"v_outward_apex\"].where(cls[\"aligned_outward\"])\n",
    "# Symmetric color scale around zero for a balanced diverging map\n",
    "vmax = np.nanpercentile(np.abs(signed_speed), 98)  # robust cap\n",
    "norm = TwoSlopeNorm(vmin=-vmax, vcenter=0.0, vmax=vmax)\n",
    "plt.figure()\n",
    "signed_speed.plot(cmap=\"RdBu_r\", norm=norm)\n",
    "plt.title(\"Front pixels aligned with outward-from-apex — signed speed (m/day)\\n(negative=inward, positive=outward)\")\n",
    "plt.xlabel(\"Longitude\"); plt.ylabel(\"Latitude\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15888db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install earthengine-api geemap geopandas shapely pyproj --quiet\n",
    "# Initialize EE (authenticate once per environment)\n",
    "try:\n",
    "    ee.Initialize()\n",
    "except Exception:\n",
    "    ee.Authenticate()  # follow the printed URL once\n",
    "    ee.Initialize()\n",
    "# Infer lat/lon names from your DataArray\n",
    "lat_name = \"lat\" if \"lat\" in da.coords else (\"latitude\" if \"latitude\" in da.coords else \"y\")\n",
    "lon_name = \"lon\" if \"lon\" in da.coords else (\"longitude\" if \"longitude\" in da.coords else \"x\")\n",
    "# AOI = bounding box of your data (tighten if desired)\n",
    "lat_min = float(da[lat_name].min())\n",
    "lat_max = float(da[lat_name].max())\n",
    "lon_min = float(da[lon_name].min())\n",
    "lon_max = float(da[lon_name].max())\n",
    "aoi = ee.Geometry.Rectangle([lon_min, lat_min, lon_max, lat_max])\n",
    "print(\"AOI:\", (lat_min, lon_min, lat_max, lon_max))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b65eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your chosen dataset & filter\n",
    "fc_riv = (ee.FeatureCollection('WWF/HydroSHEDS/v1/FreeFlowingRivers')\n",
    "          .filter(ee.Filter.lte('RIV_ORD', 7))\n",
    "          .filterBounds(aoi))\n",
    "# Convert to GeoDataFrame (WGS84) on the client\n",
    "gdf_channels = geemap.ee_to_gdf(fc_riv)\n",
    "print(f\"Downloaded {len(gdf_channels)} channel features from EE.\")\n",
    "gdf_channels = gdf_channels[~gdf_channels.geometry.is_empty].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f3d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose two dates in your record (example placeholders)\n",
    "t1 = \"2019-05-15\"\n",
    "t2 = \"2019-06-01\"\n",
    "# Pick a meaningful front threshold if your variable is 0–1 (e.g., 0.5); set None to auto gradient band\n",
    "ds_perp = front_normal_velocity(\n",
    "    da, t1, t2,\n",
    "    front_value=0.5,    # <-- adjust to your metric; or use None\n",
    "    bandwidth=0.05,\n",
    "    smooth_px=1\n",
    ")\n",
    "ds_perp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc2566f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- small helpers ----------\n",
    "# ---------- main function ----------\n",
    "# ---------- example usage ----------\n",
    "# gdf_channels = gpd.read_file(\"channels_okavango.geojson\")  # your lines\n",
    "# ds_perp = front_normal_velocity(da, \"2019-05-15\", \"2019-06-01\", front_value=0.5, \n",
    "# bandwidth=0.05, smooth_px=1)\n",
    "\n",
    "\n",
    "# Set the delta apex so “positive” is outward/downstream (RECOMMENDED)\n",
    "# Replace with your preferred apex (top of Okavango Delta, approx ~(-19.0, 22.4) — tune if needed)\n",
    "apex = (-19.0, 22.4)\n",
    "gdf_speed = front_speed_along_channels(\n",
    "    ds_perp,\n",
    "    gdf_channels,\n",
    "    sample_spacing_m=50.0,      # ~1–3× your pixel size works well\n",
    "    apex_latlon=apex,            # ensures sign = outward/downstream along each line\n",
    "    require_front_mask=True,     # only where the front exists\n",
    "    min_speed_m_per_day=10.0     # ignore near-zero (angle-unstable) estimates\n",
    ")\n",
    "gdf_speed.head()\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "# Per-reach median speed\n",
    "reach_stats = (gdf_speed\n",
    "               .dropna(subset=[\"v_along_m_per_day\"])\n",
    "               .groupby(\"line_id\")[\"v_along_m_per_day\"]\n",
    "               .median()\n",
    "               .rename(\"median_v_along_m_per_day\"))\n",
    "# Attach the median back to the original channel geometry (same order as iterated)\n",
    "reach_gdf = gpd.GeoDataFrame(\n",
    "    {\"line_id\": reach_stats.index, \"median_v_along_m_per_day\": reach_stats.values},\n",
    "    geometry=gdf_channels.geometry.iloc[reach_stats.index].values,\n",
    "    crs=gdf_channels.crs\n",
    ")\n",
    "# Map: channels grey; color by along-channel median speed (m/day)\n",
    "ax = gdf_channels.plot(color=\"lightgray\", linewidth=0.6)\n",
    "reach_gdf.plot(ax=ax, column=\"median_v_along_m_per_day\", legend=True, linewidth=2)\n",
    "plt.title(f\"Front speed along channel network (m/day)\\n{ds_perp.attrs['t1']} → {ds_perp.attrs['t2']}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c04901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfe9c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming we already have:\n",
    "#   ds_perp = front_normal_velocity(da, \"2019-05-15\", \"2019-06-15\",\n",
    "#                                   front_value=0.5, bandwidth=0.05, smooth_px=1)\n",
    "#   gdf_channels from HydroSHEDS (or your own network)\n",
    "\n",
    "px = velocity_parallel_to_nearest_channel_field(\n",
    "        ds_perp, gdf_channels,\n",
    "        sample_spacing_m=500.0,   # ~ pixel size or a bit smaller\n",
    "        apex_latlon=apex_ll,\n",
    "        require_front_mask=True,\n",
    "        max_distance_m=3000.0     # ignore pixels > 3 km from a channel (optional)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac77bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.colors import TwoSlopeNorm\n",
    "# # Signed parallel component (m/day) where we computed it\n",
    "# mask = np.isfinite(px[\"v_parallel\"])\n",
    "# v = px[\"v_parallel\"].where(mask)\n",
    "# vmax = np.nanpercentile(np.abs(v), 98)  # robust symmetric scale\n",
    "# norm = TwoSlopeNorm(vmin=-vmax, vcenter=0.0, vmax=vmax)\n",
    "# plt.figure()\n",
    "# v.plot(cmap=\"RdBu_r\", norm=norm)\n",
    "# plt.title(f\"Front motion parallel to nearest channel (m/day)\\n{ds_perp.attrs['t1']} → {ds_perp.attrs['t2']}\")\n",
    "# # (Optional) show how far we were from a channel when computing\n",
    "# # plt.figure()\n",
    "# # px[\"dist_to_channel_m\"].plot()\n",
    "# # plt.title(\"Distance to nearest channel sample (m)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acb0656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- choose coord names from px (works for lat/lon or latitude/longitude) ---\n",
    "# lat_name = next(n for n in (\"lat\",\"latitude\",\"y\") if n in px.coords)\n",
    "# lon_name = next(n for n in (\"lon\",\"longitude\",\"x\") if n in px.coords)\n",
    "# # Raster extent\n",
    "# lon_min, lon_max = float(px[lon_name].min()), float(px[lon_name].max())\n",
    "# lat_min, lat_max = float(px[lat_name].min()), float(px[lat_name].max())\n",
    "# # Channels to WGS84\n",
    "# channels_wgs84 = gdf_channels.to_crs(epsg=4326)\n",
    "# # --- build bbox polygon with shapely ---\n",
    "# bbox_poly = box(lon_min, lat_min, lon_max, lat_max)\n",
    "# bbox_gdf  = gpd.GeoDataFrame(geometry=[bbox_poly], crs=\"EPSG:4326\")\n",
    "# # --- clip channels to raster extent (with robust fallback) ---\n",
    "# try:\n",
    "#     channels_clipped = gpd.clip(channels_wgs84, bbox_gdf)\n",
    "# except Exception:\n",
    "#     # Fallback: filter by intersection, then actually intersect to trim to bbox\n",
    "#     sel = channels_wgs84.intersects(bbox_poly)\n",
    "#     channels_clipped = channels_wgs84.loc[sel].copy()\n",
    "#     channels_clipped[\"geometry\"] = channels_clipped.geometry.intersection(bbox_poly)\n",
    "# # --- pick what to show: signed parallel component, optionally within N meters of channels ---\n",
    "# v = px[\"v_parallel\"]  # >0 outward/downstream if you set apex in the computation\n",
    "# max_dist_m = 3000.0\n",
    "# if \"dist_to_channel_m\" in px:\n",
    "#     v = v.where(px[\"dist_to_channel_m\"] <= max_dist_m)\n",
    "# # Robust symmetric color scale around 0\n",
    "# vmax = float(np.nanpercentile(np.abs(v.values), 98))\n",
    "# norm = TwoSlopeNorm(vmin=-vmax, vcenter=0.0, vmax=vmax)\n",
    "# # --- plot ---\n",
    "# plt.figure(figsize=(9, 8))\n",
    "# im = v.plot(cmap=\"RdBu_r\", norm=norm)  # raster first\n",
    "# cbar = im.colorbar\n",
    "# cbar.set_label(\"Parallel front speed (m/day)\")\n",
    "# # Draw channels on top (weight by order if available)\n",
    "# if \"RIV_ORD\" in channels_clipped.columns:\n",
    "#     lw = 0.4 + 0.25 * channels_clipped[\"RIV_ORD\"].clip(upper=8)  # small linewidth variation\n",
    "#     channels_clipped.plot(linewidth=lw, color=\"black\", alpha=0.85)\n",
    "# else:\n",
    "#     channels_clipped.plot(linewidth=0.8, color=\"black\", alpha=0.85)\n",
    "# plt.xlim(lon_min, lon_max); plt.ylim(lat_min, lat_max)\n",
    "# title_dates = f\"{ds_perp.attrs.get('t1','')} → {ds_perp.attrs.get('t2','')}\"\n",
    "# plt.title(f\"Front motion parallel to nearest channel (m/day)\\n{title_dates}\")\n",
    "# plt.xlabel(\"Longitude\"); plt.ylabel(\"Latitude\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727194a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- example call (adjust as needed) ----\n",
    "title_dates = f\"{ds_perp.attrs.get('t1','')} → {ds_perp.attrs.get('t2','')}\"\n",
    "apex = (-19.0, 22.4)\n",
    "plot_parallel_velocity_with_channels(px, gdf_channels, apex=apex,\n",
    "                                     max_dist_m=2000000.0, linewidth_by_order=True,\n",
    "                                     title_dates=title_dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c2e686",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ee-map)",
   "language": "python",
   "name": "ee-map"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
